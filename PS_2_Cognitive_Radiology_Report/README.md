# Cognitive Radiology Assistant: Automated Medical Report Generation

**Team:** monikabhati2005 | **Hackathon:** BrainDead 2026 | **Problem Statement:** PS_2

---

## ğŸ¯ Project Overview

The Cognitive Radiology Assistant generates comprehensive diagnostic reports from chest X-rays using a **vision-language model** with three mandatory modules:

1. **PRO-FA (Progressive Feature Alignment):** Hierarchical visual encoding at pixel, region, and organ levels
2. **MIX-MLP (Multi-task Knowledge-Enhanced MLP):** 14-disease classification with pathology predictions
3. **RCTA (Recursive Cognitive Triangular Attention):**Tri-modal attention fusing image, clinical indication, and disease features

---

## ğŸ—ï¸ System Architecture

```mermaid
flowchart TD
    Input["ğŸ¥ INPUT: Chest X-Ray (224Ã—224)"] --> PROFA
    
    PROFA["ğŸ“¸ MODULE 1: PRO-FA<br/>(Hierarchical Visual Alignment)<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Vision Transformer (ViT-B/16)<br/>â†’ Multi-Granular Features<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>â€¢ Pixel-level: 196 patches Ã— 512-dim<br/>â€¢ Region-level: 49 regions Ã— 512-dim<br/>â€¢ Organ-level: 1 CLS token Ã— 512-dim"] --> MIXMLP
    
    MIXMLP["ğŸ”¬ MODULE 2: MIX-MLP<br/>(Knowledge-Enhanced Classification)<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Organ features â†’ MLP â†’ 14 Pathology Scores<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Diseases: No Finding, Cardiomegaly, Edema,<br/>Consolidation, Pneumonia, Atelectasis,<br/>Pneumothorax, Pleural Effusion, ...<br/>Output: p âˆˆ [0,1]^14"] --> Disease["ğŸ§¬ Disease Embeddings<br/>(Linear projection<br/>to 512-dim)"]
    
    PROFA --> ImageFeat["ğŸ–¼ï¸ Image Features"]
    
    Clinical["ğŸ’¬ Clinical Indication<br/>(DistilBERT encoding)"] --> RCTA
    ImageFeat --> RCTA
    Disease --> RCTA
    
    RCTA["ğŸ”º MODULE 3: RCTA<br/>(Recursive Cognitive Triangular Attention)<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Tri-Modal Attention<br/>Image âŠ— Clinical âŠ— Disease<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Q_Image â†’ Attend to<br/>[Clinical Indication, Disease Predictions]<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Verified Features =<br/>LayerNorm(Image + Attn_Clinical + Attn_Disease)"] --> Decoder
    
    Decoder["ğŸ“ DECODER: DistilGPT2<br/>with Cross-Attention<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Verified Features â†’ GPT-2 Generate<br/>(Beam Search, k=4)<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Output: 'FINDINGS: The cardiomediastinal<br/>silhouette is normal...'"]
    
    style PROFA fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    style MIXMLP fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px
    style RCTA fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    style Decoder fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
    style Input fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    style Clinical fill:#fff9c4,stroke:#f9a825,stroke-width:2px
    style Disease fill:#f1f8e9,stroke:#689f38,stroke-width:2px
    style ImageFeat fill:#e0f7fa,stroke:#00acc1,stroke-width:2px
```


---

## ğŸ“ Project Structure

```
PS_2_Cognitive_Radiology_Report/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ encoder.py               # MODULE 1: PRO-FA implementation
â”‚   â”œâ”€â”€ classifier.py            # MODULE 2: MIX-MLP implementation
â”‚   â”œâ”€â”€ attention.py             # MODULE 3: RCTA implementation
â”‚   â”œâ”€â”€ cognitive_model.py       # Complete integrated model
â”‚   â””â”€â”€ dataset.py               # ChestXrayDataset, data transforms
â”œâ”€â”€ training/
â”‚   â””â”€â”€ train.py                 # Training loop with checkpointing
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ evaluate.py              # CheXpert F1, RadGraph F1, CIDEr, BLEU-4
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_data.sh         # Dataset download scripts
â”‚   â”œâ”€â”€ download_with_hub.py     # HuggingFace dataset download
â”‚   â”œâ”€â”€ download_mimic.py        # MIMIC-CXR download
â”‚   â”œâ”€â”€ preprocess_iu_xray.py    # IU X-Ray preprocessing
â”‚   â”œâ”€â”€ preprocess_mimic.py      # MIMIC-CXR preprocessing
â”‚   â”œâ”€â”€ generate_reports.py      # Inference script
â”‚   â””â”€â”€ gui_app.py               # Interactive GUI demo (PySide6)
â”œâ”€â”€ data/                        # Dataset directory (git-ignored)
â”œâ”€â”€ checkpoints/                 # Trained model weights
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ Run_on_Colab.ipynb       # Google Colab training notebook
â”‚   â””â”€â”€ Run_on_Kaggle.ipynb      # Kaggle P100 GPU notebook
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ pack_project.py              # Zip codebase for submission
â”œâ”€â”€ submission.txt               # Submission links file
â””â”€â”€ README.md                    # This file
```

---

## ğŸš€ Quick Start

### Option 1: Google Colab (Recommended for Training)

**Best for:** Training on GPU without local setup

1. **Prerequisites:**
   - Google account for Colab
   - Kaggle account + API token (`kaggle.json`)

2. **Steps:**
   ```bash
   # Download project codebase
   # Go to Google Colab: https://colab.research.google.com/
   # Upload Run_on_Colab.ipynb
   # Upload project_codebase.zip and kaggle.json
   # Run all cells
   ```

3. **What it does:**
   - Installs dependencies
   - Downloads IU X-Ray or MIMIC-CXR
   - Trains model for 15 epochs
   - Saves checkpoint to `/content/checkpoints/`

### Option 2: Local Setup (For Inference/GUI Demo)

**Best for:** Running pre-trained model and GUI demo

```bash
# 1. Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 2. Install dependencies
pip install -r requirements.txt

# 3. Run GUI Demo
python scripts/gui_app.py
```

**GUI Features:**
- Load trained checkpoint
- Upload chest X-ray
- Execute diagnostic scan with visual animations
- View generated report + disease probabilities
- Professional medical interface

### Option 3: Kaggle Notebooks (P100 GPU)

**Best for:** Fast training with more memory

1. Upload `Run_on_Kaggle.ipynb` to Kaggle
2. Enable GPU accelerator (Settings â†’ Accelerator â†’ GPU P100)
3. Add IU X-Ray or MIMIC-CXR dataset
4. Run all cells

---

## ğŸ“Š Datasets

### IU X-Ray (Indiana University)
- **Images:** 7,470 frontal chest X-rays
- **Reports:** Paired radiology reports with findings & impressions
- **Diseases:** 14 CheXpert pathology labels

### MIMIC-CXR (MIT)
- **Images:** 377,110 chest X-rays (227K patients)
- **Reports:** De-identified radiology reports
- **Diseases:** Same 14 CheXpert labels
- **Challenge:** Nested directory structure (handled in dataset.py)

**Download:**
```bash
# Via Kaggle API (fastest)
python scripts/download_with_hub.py  # IU X-Ray
python scripts/download_mimic.py     # MIMIC-CXR

# Manual: Download from Kaggle datasets and place in data/
```

---

## ğŸ§  Module Implementation Details

### MODULE 1: PRO-FA (Hierarchical Visual Alignment)

**File:** [`models/encoder.py`](models/encoder.py)

```python
class PROFA(nn.Module):
    """
    Progressive Feature Alignment extracts multi-scale visual features
    from a pre-trained Vision Transformer (ViT-B/16).
    """
    def forward(self, images):
        # Extract ViT features [B, 197, 768]
        features = self.vit.forward_features(images)
        
        # Level 1: Organ (global context from CLS token)
        organ_feat = self.organ_proj(features[:, 0])  # [B, 512]
        
        # Level 2: Region (7Ã—7 pooled patches)
        patch_grid = features[:, 1:].reshape(B, 14, 14, 768)
        region_feat = avg_pool2d(patch_grid, kernel_size=2)  # [B, 49, 512]
        
        # Level 3: Pixel (all 196 patches)
        pixel_feat = self.pixel_proj(features[:, 1:])  # [B, 196, 512]
        
        return {'pixel': pixel_feat, 'region': region_feat, 'organ': organ_feat}
```

**Key Design Choice:** Multi-granular features allow the model to capture both fine anatomical details (pixels) and global pathology context (organ), mimicking radiologist analysis.

### MODULE 2: MIX-MLP (Multi-task Knowledge-Enhanced MLP)

**File:** [`models/classifier.py`](models/classifier.py)

```python
class MIXMLP(nn.Module):
    """
    Multi-Layer Perceptron for 14-disease classification.
    Uses class weighting to handle label imbalance."""
    def forward(self, organ_features):
        # organ_features: [B, 512]
        x = F.relu(self.fc1(organ_features))       # [B, 256]
        x = self.dropout(x)
        x = F.relu(self.fc2(x))                    # [B, 128]
        logits = self.fc3(x)                       # [B, 14]
        return logits  # Apply sigmoid for probabilities: p = Ïƒ(logits)
```

**Loss:** Binary cross-entropy with class weights to address dataset imbalance (e.g., "No Finding" is more common than "Pneumothorax").

### MODULE 3: RCTA (Recursive Cognitive Triangular Attention)

**File:** [`models/attention.py`](models/attention.py)

```python
class RCTA(nn.Module):
    """
    Triangular attention mechanism that fuses:
    - Image features (from PRO-FA)
    - Clinical indication (from text encoder)
    - Disease predictions (from MIX-MLP)
    """
    def forward(self, img_feat, clinical_feat, disease_feat):
        # Image as Query, Clinical/Disease as Key-Value
        Q_img = self.q_proj(img_feat)
        
        # Attend to clinical context
        K_clin, V_clin = self.kv_clin(clinical_feat)
        attn_clin = softmax(Q_img @ K_clin.T / âˆšd_k) @ V_clin
        
        # Attend to disease predictions
        K_dis, V_dis = self.kv_disease(disease_feat)
        attn_dis = softmax(Q_img @ K_dis.T / âˆšd_k) @ V_dis
        
        # Residual connection + LayerNorm
        return LayerNorm(img_feat + attn_clin + attn_dis)
```

**Innovation:** Unlike standard cross-attention, RCTA explicitly models the cognitive loop: image perception â†’ disease hypothesis â†’ clinical reasoning.

---

## ğŸ¯ Training

### Basic Training

```bash
python training/train.py \
  --dataset iu_xray \
  --batch_size 8 \
  --epochs 15 \
  --lr 1e-4 \
  --device cuda
```

### Advanced Options

```bash
# Resume from checkpoint
python training/train.py --resume --checkpoint checkpoints/epoch_10.pth

# Mock data mode (for pipeline testing)
python training/train.py --mock_data --epochs 1

# Custom data paths
python training/train.py \
  --data_csv data/iu_xray/train.csv \
  --img_dir data/iu_xray/images
```

**Training time:** ~2-3 hours on P100 GPU for 15 epochs (IU X-Ray)

---

## ğŸ“ˆ Evaluation Metrics

The model is evaluated on:

| Metric Category | Metric | Target | Purpose |
|-----------------|--------|--------|---------|
| **Clinical Accuracy** | CheXpert F1 | > 0.500 | Disease classification precision |
| **Structural Logic** | RadGraph F1 | > 0.500 | Entity-relation extraction |
| **NLG Fluency** | CIDEr | > 0.400 | Text quality & keyword coverage |
| **NLG Fluency** | BLEU-4 | â€” | N-gram overlap with reference |

**Run evaluation:**
```bash
python evaluation/evaluate.py \
  --checkpoint checkpoints/best_model.pth \
  --test_csv data/iu_xray/test.csv
```

---

## ğŸ–¥ï¸ GUI Demo Application

The interactive GUI (`scripts/gui_app.py`) features:

- **Professional Interface:** Medical-grade design with team branding
- **Neural Ribbon Loader:** Gradient-animated progress during weight loading
- **Medical Heartbeat Pulse:** Pulsating glow on X-ray during inference
- **14-Segment Disease Rail:** Visual indicator lighting up for each classification head
- **Circular Progress Ring:** Completion tracker on scan button
- **Staggered Result Reveal:** Cinematic fade-in of findings with color-coded risk levels
- **High-Risk Highlighting:** Pulsing shadows on findings >80% confidence

**Launch:**
```bash
python scripts/gui_app.py
```

**Demo Video Script:** See [`DEMO_VIDEO_SCRIPT.md`](DEMO_VIDEO_SCRIPT.md) for 2-minute screencast guide.

---

## ğŸ† Hackathon Evaluation Criteria

| Criterion | Weight | Implementation | Evidence |
|-----------|--------|----------------|----------|
| **Module Implementation** | 40% | âœ… All 3 modules | encoder.py, classifier.py, attention.py |
| **Clinical F1 Score** | 30% | Ready for evaluation | evaluate.py framework |
| **Code Quality** | 15% | âœ… Modular, documented | See code comments |
| **Innovation** | 15% | âœ… Tri-modal attention, GUI | Unique RCTA design |

**Submission Checklist:**
- âœ… PRO-FA, MIX-MLP, RCTA implemented
- âœ… Training pipeline complete
- âœ… Evaluation framework ready
- âœ… Professional GUI demo
- âœ… Comprehensive documentation

---

## ğŸ”§ Troubleshooting

### CUDA Out of Memory
```bash
# Reduce batch size
python training/train.py --batch_size 4

# Or use gradient accumulation (if implemented)
```

### Tokenizer Mismatch Errors
```bash
# Ensure correct tokenizers are installed
pip install transformers==4.30.0
```

### Dataset Not Found
```bash
# Verify data paths in train.py or use absolute paths
python training/train.py --img_dir /absolute/path/to/images
```

### GUI Not Launching
```bash
# Ensure PySide6 is installed
pip install PySide6==6.5.0
```

---

## ğŸ“š References

**Datasets:**
- IU X-Ray: Demner-Fushman et al., "Preparing a collection of radiology examinations for distribution and retrieval," JAMIA, 2016
- MIMIC-CXR: Johnson et al., "MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs," arXiv, 2019

**Metrics:**
- CheXpert: Irvin et al., "CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels," AAAI, 2019
- RadGraph: Jain et al., "RadGraph: Extracting Clinical Entities and Relations from Radiology Reports," NeurIPS Datasets, 2021

---

## ğŸ“ License & Citation

```bibtex
@misc{cognitive2026,
  title={Cognitive Radiology Assistant: Automated Medical Report Generation},
  author={Team monikabhati2005},
  year={2026},
  note={BrainDead Hackathon Submission - Implements PRO-FA, MIX-MLP, RCTA modules}
}
```

---

**Contact:** monikabhati2005 | **Repository:** [GitHub Link TBD] | **Demo:** [Video Link TBD]
